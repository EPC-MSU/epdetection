{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee26c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor,Resize, Grayscale, Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "zip_file = \"/content/gdrive/MyDrive/base_v2.1.zip\"\n",
    "z = zipfile.ZipFile(zip_file, 'r')\n",
    "z.extractall()\n",
    "print(os.listdir())\n",
    "\n",
    "root_dir = \"base_v2.1\"\n",
    "train_data = VisionDataset(root = os.path.join(root_dir,'train_x1'),\n",
    "                         transform= transforms)\n",
    "\"\"\"\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This import also use in detect_nn.py\n",
    "from model import CNN, transforms\n",
    "# If you want change torch model, change model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe2c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCBDataset(Dataset):\n",
    "    def __init__(self, classes_paths, transforms=None, target_transform=None):\n",
    "        #print(glob.glob(data_dir))\n",
    "        self.classes = []\n",
    "        self.classes_paths = classes_paths\n",
    "        self.images = []\n",
    "        self.transform = transforms\n",
    "        for actual_class in classes_paths:\n",
    "            if actual_class == 1:\n",
    "                print(\"\")\n",
    "            paths = classes_paths[actual_class]\n",
    "            for path in paths:\n",
    "                # print(\"Loading \" + path)\n",
    "                for top, dirs, files in os.walk(path):\n",
    "                    for i, name in enumerate(files):\n",
    "                        if not os.path.isfile(top + \"//\" + name):\n",
    "                            continue\n",
    "                        try:\n",
    "                            imag = cv2.imread(top + \"//\" + name)\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                        self.images.append(imag)\n",
    "                        self.classes.append(actual_class)\n",
    "        self.images = np.array(self.images, dtype=object)\n",
    "        self.classes = torch.tensor(np.array(self.classes), dtype=torch.int64)\n",
    "        self.plot_disbalance()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform==None:\n",
    "            return self.images[idx], self.classes[idx]\n",
    "        else:\n",
    "            return self.transform(np.array(self.images[idx]).astype(np.float32)), self.classes[idx]\n",
    "        \n",
    "    def plot_disbalance(self):\n",
    "        heights = []\n",
    "        labels = []\n",
    "        incorrect_sum = 0\n",
    "        for path in self.classes_paths[0]:\n",
    "            incorrect_sum += len(glob.glob(os.path.join(path, '*')))\n",
    "        heights.append(incorrect_sum)\n",
    "        labels.append('incorrect')\n",
    "        for i in range(1, len(self.classes_paths)-1, 1):\n",
    "            path = os.path.normpath(self.classes_paths[i][0])\n",
    "            labels.append(path.split(os.sep)[2])\n",
    "            heights.append(len(glob.glob(os.path.join(self.classes_paths[i][0], '*'))))\n",
    "        plt.bar(range(len(heights)), heights)\n",
    "        plt.ylabel('count images')\n",
    "        plt.xticks(range(len(heights)), labels, rotation='vertical')\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d67797",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_PATHS = {\n",
    "    0: [\"base_v2.1//train_x1//2-SMD//incorrect\",\n",
    "        \"base_v2.1//train_x1//SMA//incorrect\",\n",
    "        \"base_v2.1//train_x1//SMB//incorrect\",\n",
    "#         \"base_v2.1//train_x1//SOD110//incorrect\",\n",
    "#             \"base_v2.1//train_x1//SOD323F//incorrect\",\n",
    "#             \"base_v2.1//train_x1//SOD523//incorrect\",\n",
    "#             \"base_v2.1//train_x1//SOT23-5//incorrect\",\n",
    "#             \"base_v2.1//train_x1//SOT23-6//incorrect\",\n",
    "#             \"base_v2.1//train_x1//SOT143//incorrect\",\n",
    "#             \"base_v2.1//train_x1//SOT323//incorrect\",\n",
    "#             \"base_v2.1//train_x1//SOT323-5//incorrect\",\n",
    "#             \"base_v2.1//train_x1//SOT343//incorrect\",\n",
    "#             \"base_v2.1//train_x1//SOT363//incorrect\",\n",
    "#             \"base_v2.1//train_x1//SOT523//incorrect\",\n",
    "#             \"base_v2.1//train_x1//SOT723//incorrect\",\n",
    "#             \"base_v2.1//train_x1//SMD0402_CL//incorrect\",\n",
    "#             \"base_v2.1//train_x1//SMD0402_R//incorrect\",\n",
    "#             \"base_v2.1//train_x1//SMD0603_CL//incorrect\",\n",
    "#             \"base_v2.1//train_x1//SMD0603_D//incorrect\",\n",
    "#             \"base_v2.1//train_x1//SMD0603_R//incorrect\",\n",
    "#             \"base_v2.1//train_x1//SMD0805_CL//incorrect\",\n",
    "#         \"base_v2.1//train_x1//SMD0805_R//incorrect\",\n",
    "        \"base_v2.1//train_x1//SMD1206_C//incorrect\",\n",
    "        \"base_v2.1//train_x1//SMD1206_R//incorrect\",\n",
    "        \"base_v2.1//train_x1//SMD1210_C//incorrect\"],\n",
    "    1: [\"base_v2.1//train_x1//SMD0402_CL//correct\"],\n",
    "    2: [\"base_v2.1//train_x1//SMD0402_R//correct\"],\n",
    "    3: [\"base_v2.1//train_x1//SMD0603_CL//correct\"],\n",
    "    4: [\"base_v2.1//train_x1//SMD0603_D//correct\"],\n",
    "    5: [\"base_v2.1//train_x1//SMD0603_R//correct\"],\n",
    "    6: [\"base_v2.1//train_x1//SMD0805_CL//correct\"],\n",
    "    7: [\"base_v2.1//train_x1//SMD0805_R//correct\"],\n",
    "    8: [\"base_v2.1//train_x1//SMD1206_C//correct\"],\n",
    "    9: [\"base_v2.1//train_x1//SMD1206_R//correct\"],\n",
    "    10: [\"base_v2.1//train_x1//SMD1210_C//correct\"],\n",
    "    11: [\"base_v2.1//train_x1//2-SMD//correct\"],\n",
    "    12: [\"base_v2.1//train_x1//SMA//correct\"],\n",
    "    13: [\"base_v2.1//train_x1//SMB//correct\"],\n",
    "    14: [\"base_v2.1//train_x1//SOD110//correct\"],\n",
    "    15: [\"base_v2.1//train_x1//SOD323F//correct\"],\n",
    "    16: [\"base_v2.1//train_x1//SOD523//correct\"],\n",
    "    17: [\"base_v2.1//train_x1//SOT23-5//correct\"],\n",
    "    18: [\"base_v2.1//train_x1//SOT23-6//correct\"],\n",
    "    19: [\"base_v2.1//train_x1//SOT143//correct\"],\n",
    "    20: [\"base_v2.1//train_x1//SOT323//correct\"],\n",
    "    21: [\"base_v2.1//train_x1//SOT323-5//correct\"],\n",
    "    22: [\"base_v2.1//train_x1//SOT343//correct\"],\n",
    "    23: [\"base_v2.1//train_x1//SOT363//correct\"],\n",
    "    24: [\"base_v2.1//train_x1//SOT523//correct\"],\n",
    "    25: [\"base_v2.1//train_x1//SOT723//correct\"],\n",
    "    26: [\"base_v2.1//train_x1//DIP-%d//correct\"],\n",
    "    27: [\"base_v2.1//train_x1//LQFP0.4-%d//correct\"],\n",
    "    28: [\"base_v2.1//train_x1//LQFP0.5-%d&TFSOP-%d//correct\"],\n",
    "    29: [\"base_v2.1//train_x1//LQFP0.8-%d//correct\"],\n",
    "    30: [\"base_v2.1//train_x1//LQFP0.65-%d&SSOP-%d//correct\"],\n",
    "    31: [\"base_v2.1//train_x1//SOIC-%d//correct\"]\n",
    "}\n",
    "\n",
    "dataset = PCBDataset(CLASSES_PATHS, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07501b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model,testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "def test(model, loss_function, epochs , tag = \"detection\"):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_test = []\n",
    "    for epoch in range(epochs):\n",
    "        ep_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                images, labels = data\n",
    "          # calculate outputs by running images through the network\n",
    "                outputs = model(images)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                ep_loss += loss.item()\n",
    "          # the class with the highest energy is what we choose as prediction\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        loss_test.append(ep_loss /len(test_loader))\n",
    "        print(f\"Epoch={epoch} loss={loss_test[epoch]:.4}\")\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "    return loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eaa23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_function, optimizer, epochs):\n",
    "    loss_hist = []\n",
    "    test_accuracy = []\n",
    "    train_accuracy = []\n",
    "    for epoch in range(epochs):\n",
    "        ep_loss = 0\n",
    "        for images, labels in  tqdm(train_loader): # get bacth\n",
    "            optimizer.zero_grad() # sets the gradients of all optimized tensors to zero.\n",
    "            outputs = model.forward(images) # call forward inside\n",
    "            loss = loss_function(outputs, labels) # calculate loss\n",
    "            loss.backward() # calculate gradients\n",
    "            optimizer.step() # performs a single optimization step (parameter update).\n",
    "            ep_loss += loss.item()\n",
    "        loss_hist.append(ep_loss /len(train_loader))\n",
    "        print(f\"Epoch={epoch} loss={loss_hist[epoch]:.4}\")\n",
    "        test_accuracy.append(validate(model, test_loader))\n",
    "        train_accuracy.append(validate(model, train_loader))\n",
    "    return test_accuracy,train_accuracy, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d70349",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = 0.9\n",
    "dataset_train, dataset_test = torch.utils.data.random_split(dataset, [int(len(dataset) * param),\n",
    "                                                                      len(dataset) - int(len(dataset) * param)])\n",
    "train_loader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94a574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124eb3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42278a35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "EPOCHS = 10\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "test_accuracy, train_accuracy, loss_hist = train(model, criterion, optimizer, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f073af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_by_class = {}\n",
    "if 1 not in acc_by_class:\n",
    "    acc_by_class[1] = 'fdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9917d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_by_class(model, dataset_test):\n",
    "    acc_by_class = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for image, label in dataset_test:\n",
    "            outputs = model(image.unsqueeze(0))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted.squeeze() == label).sum().item()\n",
    "            \n",
    "            if int(label) not in acc_by_class:\n",
    "                acc_by_class[int(label)] = (correct, 1)\n",
    "            if int(label) in acc_by_class:\n",
    "                correct_prev, total_prev = acc_by_class[int(label)]\n",
    "                acc_by_class[int(label)] = (correct_prev + correct, total_prev + 1)\n",
    "    return acc_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = validate_by_class(model, dataset_test)\n",
    "# for key in CLASSES_PATHS.keys():\n",
    "#     try:\n",
    "#         print(CLASSES_PATHS[key], d[key])\n",
    "#     except:\n",
    "#         print(CLASSES_PATHS[key], 'NOT FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ce646",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(EPOCHS), train_accuracy, label='train accuracy')\n",
    "plt.plot(range(EPOCHS), test_accuracy, label='test accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(EPOCHS), loss_hist, label='train loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c02c1d3",
   "metadata": {},
   "source": [
    "#### Save model and try to load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fe1aff",
   "metadata": {},
   "source": [
    "Эта сложная конструкция ниже предназначена для сохранения нейронки и файла с метаинформацией, по которой можно правильно загрузить нейронку в epdetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a72d9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from model_info.model_info import ModelInfo\n",
    "\n",
    "with open(\"model_info/compatible_det_info.json\") as f:\n",
    "    model_info = ModelInfo(\"Andrey Marakulin\", json.load(f))\n",
    "    model_info.compile_info()\n",
    "    model_info.save_info()\n",
    "    model_name = model_info.get_info()['modelname']\n",
    "    \n",
    "torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb52760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test\n",
    "model_test = CNN()\n",
    "model_test.load_state_dict(torch.load(model_name))\n",
    "model_test.eval()\n",
    "model_test(torch.rand((1, 1, 32, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1decf59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
